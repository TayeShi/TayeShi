# 机器学习模型

## 分类

- 监督学习
  - 回归模型
    - 线性回归
  - 分类模型
    - k 近邻（kNN）
    - 决策树
    - 逻辑斯蒂回归
- 无监督学习
  - 聚类
    - k 均值（k-means）
  - 降维

## 回归模型

- 线性回归模型
  - 一元线性回归
  - 多元线性回归
- 非线性回归模型
- 最小二乘法

<img src="/Users/tianye/Documents/myProjects/Twinkle/待整理/机器学习/机器学习模型介绍.assets/image-20231110234355090.png" alt="image-20231110234355090" style="zoom:66%;" />

### 线性回归模型

线性回归（linear regression）是一种线性模型，它假设输入变量 x 和单个输出变量 y 之间存在线性关系
利用线性回归模型，可以从一组输入变量 x 的线性组合中，计算输出变量 y
$$
y = ax + b
$$

$$
f(\boldsymbol{x}) = w_1x_1 + w_2x_2 + \ldots + w_dx_d + b
$$

![image-20231110235626177](/Users/tianye/Documents/myProjects/Twinkle/待整理/机器学习/机器学习模型介绍.assets/image-20231110235626177.png)

给定有 d 个属性（特征）描述的示例 $x = (x1; x2; \ldots; xd)$  ，其中 $xi$是 x 在第 i 个属性（特征）上的取值，线性模型（linear model）试图学得一个通过属性（特征）的线性组合来进行预测的函数，即：
$$
f(\boldsymbol{x}) = w_1x_1 + w_2x_2 + \ldots + w_dx_d + b
$$
一般用向量形式写成：$f(\boldsymbol{x}) = \boldsymbol{w}^T\boldsymbol{x} + b$
其中 $\boldsymbol{w} = (w_1;w_2;\ldots;w_d)$

假设特征和结果都满足线性，即不大于一次方
w 和 b 学得之后，模型就得以确定
许多功能更为强大的非线性模型可在线性模型的基础上通过引入层级结构或高维映射而得

### 最小二乘法

基于均方误差最小化来进行模型求解的方法（least square method）

它主要的思想就是选择未知参数，使得理论值与观测值之差的平方和达到最小

![image-20231111002055940](/Users/tianye/Documents/myProjects/Twinkle/待整理/机器学习/机器学习模型介绍.assets/image-20231111002055940.png)

假设输入的属性（特征）的数目只有一个：
$$
f(x_i) = wx_i + b, 使得 f(x_i) \simeq y_i
$$
在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小
$$
(w^*,b^*)=\underset{(w,b)}{\arg\min} \sum_{i=1}^m(f(x_i) - y_i)^2 \\
\quad\quad\quad\quad \ =\underset{(w,b)}{\arg\min} \sum_{i=1}^m(y_i - wx_i - b)^2
$$
求解 w 和 b，使得$E_{(w, b)}=\sum_{i=1}^m(y_i - wx_i - b)^2$最小化的过程，称为线性回归模型的“最小二乘参数估计”

将$E_{(w, b)}$分别对 w 和 b 求导，可以得到![image-20231111003948906](/Users/tianye/Documents/myProjects/Twinkle/待整理/机器学习/机器学习模型介绍.assets/image-20231111003948906.png)

令偏导数都为 0，可以得到![image-20231111004018053](/Users/tianye/Documents/myProjects/Twinkle/待整理/机器学习/机器学习模型介绍.assets/image-20231111004018053.png)

其中![image-20231111004044991](/Users/tianye/Documents/myProjects/Twinkle/待整理/机器学习/机器学习模型介绍.assets/image-20231111004044991.png)

